{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPUdD4gWzo3sFbFgij3B9I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfitranurr/DATA-MINING/blob/main/Tugas_1_(Data_Teks)_Modul_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSnWrg-JOJfS",
        "outputId": "e6a7b1f4-d6de-4de7-b1de-c218c5e5dd22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QQY9jeSOXVE",
        "outputId": "251f4185-1594-4c08-8dda-216439a09b72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Persiapan Data"
      ],
      "metadata": {
        "id": "9yV0QP3uPuwh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eKGv9xJNQMP",
        "outputId": "bce88698-0005-4d8b-b8bb-527e954f57ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                               text\n",
            "0  REAL  Payal has accused filmmaker Anurag Kashyap of ...\n",
            "1  FAKE  A four-minute-long video of a woman criticisin...\n",
            "2  FAKE  Republic Poll, a fake Twitter account imitatin...\n",
            "3  REAL  Delhi teen finds place on UN green list, turns...\n",
            "4  REAL  Delhi: A high-level meeting underway at reside...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3729 entries, 0 to 3728\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   label   3729 non-null   object\n",
            " 1   text    3721 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 58.4+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Import library yang diperlukan\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import emoji\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download resource NLTK\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Membaca dataset\n",
        "data = pd.read_csv('Tugas 1_NIM Genap_Modul 4.csv')\n",
        "\n",
        "# Melihat sekilas dataset\n",
        "print(data.head())\n",
        "print(data.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Cleaning"
      ],
      "metadata": {
        "id": "w5nORPD1PykJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk membersihkan teks\n",
        "def clean_text(text):\n",
        "    # Konversi ke huruf kecil\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Menghapus emoticon\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "\n",
        "    # Mengganti tanda baca seperti - dengan spasi\n",
        "    text = text.replace('-', ' ')\n",
        "\n",
        "    # Menghapus karakter non-alfanumerik kecuali spasi\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Menghapus URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Menghapus angka\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Menghapus stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Menggabungkan kembali token\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    # Menghapus spasi berlebih\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Menerapkan pembersihan pada kolom teks\n",
        "data['cleaned_text'] = data['text'].apply(clean_text)\n",
        "\n",
        "# Memeriksa hasil pembersihan\n",
        "print(data[['text', 'cleaned_text']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLojoO9uNu-j",
        "outputId": "75ce7b56-eafc-4f86-d855-70743f24195a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  Payal has accused filmmaker Anurag Kashyap of ...   \n",
            "1  A four-minute-long video of a woman criticisin...   \n",
            "2  Republic Poll, a fake Twitter account imitatin...   \n",
            "3  Delhi teen finds place on UN green list, turns...   \n",
            "4  Delhi: A high-level meeting underway at reside...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  payal accused filmmaker anurag kashyap behavin...  \n",
            "1  four minute long video woman criticising gover...  \n",
            "2  republic poll fake twitter account imitating a...  \n",
            "3  delhi teen finds place un green list turns gla...  \n",
            "4  delhi high level meeting underway residence ra...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train Test Split"
      ],
      "metadata": {
        "id": "4ikkMEXBP1dZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengencode label (REAL=1, FAKE=0)\n",
        "label_encoder = LabelEncoder()\n",
        "data['label_encoded'] = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "# Memisahkan fitur dan label\n",
        "X = data['cleaned_text']\n",
        "y = data['label_encoded']\n",
        "\n",
        "# Membagi data menjadi train dan test (80:20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Memeriksa distribusi data\n",
        "print(f\"Ukuran X_train: {X_train.shape}\")\n",
        "print(f\"Ukuran X_test: {X_test.shape}\")\n",
        "print(f\"Distribusi label di y_train: {np.bincount(y_train)}\")\n",
        "print(f\"Distribusi label di y_test: {np.bincount(y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFObduJtNxS1",
        "outputId": "9bf5d6e0-03d7-40b6-8791-2a770aa02ce0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ukuran X_train: (2983,)\n",
            "Ukuran X_test: (746,)\n",
            "Distribusi label di y_train: [1501 1482]\n",
            "Distribusi label di y_test: [376 370]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Vektorisasi Teks"
      ],
      "metadata": {
        "id": "eXR-DsyDP5Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "\n",
        "# Fit dan transform pada data pelatihan\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "\n",
        "# Transform pada data pengujian\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# Memeriksa bentuk matriks\n",
        "print(f\"Bentuk X_train_tfidf: {X_train_tfidf.shape}\")\n",
        "print(f\"Bentuk X_test_tfidf: {X_test_tfidf.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvwS9olLNyiy",
        "outputId": "5fb900a5-e249-486e-f9b9-8d82dbf46cee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bentuk X_train_tfidf: (2983, 5000)\n",
            "Bentuk X_test_tfidf: (746, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model Naive Bayes"
      ],
      "metadata": {
        "id": "Ui6ezAJiP8nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi model Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Melatih model\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Prediksi pada data pengujian\n",
        "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluasi model\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(f\"Akurasi Naive Bayes: {accuracy_nb:.4f}\")\n",
        "print(\"\\nClassification Report Naive Bayes:\")\n",
        "print(classification_report(y_test, y_pred_nb, target_names=label_encoder.classes_))\n",
        "print(\"\\nConfusion Matrix Naive Bayes:\")\n",
        "print(confusion_matrix(y_test, y_pred_nb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DLenS5pNzhD",
        "outputId": "1c782662-4398-47ea-cfdb-30e7d82a17d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi Naive Bayes: 0.9504\n",
            "\n",
            "Classification Report Naive Bayes:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.91      1.00      0.95       376\n",
            "        REAL       1.00      0.90      0.95       370\n",
            "\n",
            "    accuracy                           0.95       746\n",
            "   macro avg       0.96      0.95      0.95       746\n",
            "weighted avg       0.95      0.95      0.95       746\n",
            "\n",
            "\n",
            "Confusion Matrix Naive Bayes:\n",
            "[[376   0]\n",
            " [ 37 333]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model ANN"
      ],
      "metadata": {
        "id": "m5Ohg1rgQBPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengubah data ke format dense untuk ANN\n",
        "X_train_dense = X_train_tfidf.toarray()\n",
        "X_test_dense = X_test_tfidf.toarray()\n",
        "\n",
        "# Membangun model ANN\n",
        "ann_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Kompilasi model\n",
        "ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Melatih model\n",
        "history = ann_model.fit(X_train_dense, y_train, epochs=10, batch_size=32,\n",
        "                       validation_data=(X_test_dense, y_test), verbose=1)\n",
        "\n",
        "# Evaluasi model\n",
        "loss, accuracy_ann = ann_model.evaluate(X_test_dense, y_test)\n",
        "print(f\"Akurasi ANN: {accuracy_ann:.4f}\")\n",
        "\n",
        "# Prediksi untuk classification report\n",
        "y_pred_ann = (ann_model.predict(X_test_dense) > 0.5).astype(int)\n",
        "print(\"\\nClassification Report ANN:\")\n",
        "print(classification_report(y_test, y_pred_ann, target_names=label_encoder.classes_))\n",
        "print(\"\\nConfusion Matrix ANN:\")\n",
        "print(confusion_matrix(y_test, y_pred_ann))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5KuW9hcN0st",
        "outputId": "b74cd2fa-9778-4d9b-8fff-b0223cc5c09e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.8995 - loss: 0.4600 - val_accuracy: 0.9946 - val_loss: 0.0170\n",
            "Epoch 2/10\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9975 - loss: 0.0130 - val_accuracy: 0.9960 - val_loss: 0.0102\n",
            "Epoch 3/10\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.9984 - loss: 0.0059 - val_accuracy: 0.9960 - val_loss: 0.0080\n",
            "Epoch 4/10\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.9987 - loss: 0.0028 - val_accuracy: 0.9960 - val_loss: 0.0069\n",
            "Epoch 5/10\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.9980 - loss: 0.0036 - val_accuracy: 0.9946 - val_loss: 0.0063\n",
            "Epoch 6/10\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9981 - loss: 0.0030 - val_accuracy: 0.9973 - val_loss: 0.0060\n",
            "Epoch 7/10\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9987 - loss: 0.0018 - val_accuracy: 0.9973 - val_loss: 0.0059\n",
            "Epoch 8/10\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9993 - loss: 0.0019 - val_accuracy: 0.9973 - val_loss: 0.0059\n",
            "Epoch 9/10\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9999 - loss: 8.1101e-04 - val_accuracy: 0.9973 - val_loss: 0.0058\n",
            "Epoch 10/10\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9973 - loss: 0.0030 - val_accuracy: 0.9946 - val_loss: 0.0060\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9960 - loss: 0.0048\n",
            "Akurasi ANN: 0.9946\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\n",
            "Classification Report ANN:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       1.00      0.99      0.99       376\n",
            "        REAL       0.99      1.00      0.99       370\n",
            "\n",
            "    accuracy                           0.99       746\n",
            "   macro avg       0.99      0.99      0.99       746\n",
            "weighted avg       0.99      0.99      0.99       746\n",
            "\n",
            "\n",
            "Confusion Matrix ANN:\n",
            "[[373   3]\n",
            " [  1 369]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Perbandingan Akurasi"
      ],
      "metadata": {
        "id": "H1FvND8_QEhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Perbandingan Akurasi\n",
        "print(\"\\nPerbandingan Akurasi:\")\n",
        "print(f\"Akurasi Naive Bayes: {accuracy_nb:.4f}\")\n",
        "print(f\"Akurasi ANN: {accuracy_ann:.4f}\")\n",
        "print(f\"Perbedaan Akurasi (ANN - Naive Bayes): {(accuracy_ann - accuracy_nb):.4f}\")\n",
        "\n",
        "# Menentukan model terbaik\n",
        "best_model = \"Naive Bayes\" if accuracy_nb > accuracy_ann else \"ANN\"\n",
        "print(f\"Model terbaik berdasarkan akurasi: {best_model}\")\n",
        "\n",
        "# Membuat tabel\n",
        "table_data = [\n",
        "    [\"Naive Bayes\", f\"{accuracy_nb:.4f}\", f\"{(accuracy_ann - accuracy_nb):.4f}\", best_model if accuracy_nb > accuracy_ann else \"\"],\n",
        "    [\"ANN\", f\"{accuracy_ann:.4f}\", \"\", best_model if accuracy_ann >= accuracy_nb else \"\"]\n",
        "]\n",
        "\n",
        "headers = [\"Model\", \"Akurasi\", \"Perbedaan Akurasi (ANN - Naive Bayes)\", \"Model Terbaik\"]\n",
        "\n",
        "# Menampilkan tabel\n",
        "print(\"\\nTabel Perbandingan Akurasi:\")\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36CNf1OpPKne",
        "outputId": "dc3659dd-337e-4e2f-e9d5-081dbcc549e4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perbandingan Akurasi:\n",
            "Akurasi Naive Bayes: 0.9504\n",
            "Akurasi ANN: 0.9946\n",
            "Perbedaan Akurasi (ANN - Naive Bayes): 0.0442\n",
            "Model terbaik berdasarkan akurasi: ANN\n",
            "\n",
            "Tabel Perbandingan Akurasi:\n",
            "+-------------+-----------+-----------------------------------------+-----------------+\n",
            "| Model       |   Akurasi | Perbedaan Akurasi (ANN - Naive Bayes)   | Model Terbaik   |\n",
            "+=============+===========+=========================================+=================+\n",
            "| Naive Bayes |    0.9504 | 0.0442                                  |                 |\n",
            "+-------------+-----------+-----------------------------------------+-----------------+\n",
            "| ANN         |    0.9946 |                                         | ANN             |\n",
            "+-------------+-----------+-----------------------------------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Contoh Prediksi"
      ],
      "metadata": {
        "id": "XSsFwLYWQGfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Teks bebas untuk prediksi\n",
        "sample_text = \"A viral video claims that the government is hiding the truth about a new disease outbreak in the capital.\"\n",
        "\n",
        "# Membersihkan teks\n",
        "cleaned_sample = clean_text(sample_text)\n",
        "\n",
        "# Vektorisasi teks\n",
        "sample_tfidf = tfidf.transform([cleaned_sample])\n",
        "sample_tfidf_dense = sample_tfidf.toarray()\n",
        "\n",
        "# Prediksi dengan Naive Bayes\n",
        "prediction_nb = nb_model.predict(sample_tfidf)\n",
        "predicted_label_nb = label_encoder.inverse_transform(prediction_nb)[0]\n",
        "\n",
        "# Prediksi dengan ANN\n",
        "prediction_ann = (ann_model.predict(sample_tfidf_dense) > 0.5).astype(int)\n",
        "predicted_label_ann = label_encoder.inverse_transform(prediction_ann.flatten())[0]\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "print(\"\\nContoh Prediksi:\")\n",
        "print(f\"Teks: {sample_text}\")\n",
        "print(f\"Prediksi Naive Bayes: {predicted_label_nb}\")\n",
        "print(f\"Prediksi ANN: {predicted_label_ann}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eIXTKKZN1rw",
        "outputId": "c0590767-6365-40da-986a-76140f579b65"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\n",
            "Contoh Prediksi:\n",
            "Teks: A viral video claims that the government is hiding the truth about a new disease outbreak in the capital.\n",
            "Prediksi Naive Bayes: FAKE\n",
            "Prediksi ANN: FAKE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-0vPUprHQhZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}